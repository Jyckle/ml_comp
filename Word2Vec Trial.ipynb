{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import unicodedata as ud\n",
    "import csv\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import training data for vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_vec_data(filename, N):\n",
    "    pre_string = \"<s>\"\n",
    "    post_string = \"</s>\"\n",
    "    train_dat = []\n",
    "        \n",
    "    with open(filename, encoding=\"utf8\") as file:\n",
    "        for line in file:\n",
    "            line = ud.normalize(\"NFC\",line)\n",
    "            line = re.sub('[,.?\"“”]','',line)\n",
    "            line = re.sub('\\s+',' ',line)\n",
    "            line = line.lower()\n",
    "            split_line = line.strip().split()\n",
    "            for gram in range(1,N):\n",
    "                split_line.insert(0,pre_string)\n",
    "                split_line.append(post_string)\n",
    "            train_dat.append(split_line)\n",
    "    return train_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_vec_data(filename):\n",
    "#     train_dat = []\n",
    "#     with open(filename, encoding=\"utf8\") as file:\n",
    "#         for line in file:\n",
    "#             line = ud.normalize(\"NFC\",line)\n",
    "#             line = re.sub('[,.?\"“”]','',line)\n",
    "#             line = re.sub('\\s+',' ',line)\n",
    "#             split_line = line.strip().split()\n",
    "#             train_dat.append(split_line)\n",
    "#     return train_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_train= prepare_vec_data('train.txt',5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Word2Vec(vec_train, size = 200, max_final_vocab=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_vocab = model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_preprocess(vec_train,set_vocab):\n",
    "    new_train_vec=[]\n",
    "    for line in vec_train:\n",
    "        new_line=[]\n",
    "        for word in line:\n",
    "            if word in set_vocab:\n",
    "                new_line.append(word)\n",
    "            else:\n",
    "                new_line.append('<UNK>')\n",
    "        new_train_vec.append(new_line)\n",
    "    return new_train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_vec = vocab_preprocess(vec_train,set_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_vec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model=Word2Vec(new_train_vec, size = 200, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data by vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to determine which vocab word an index is in the training data\n",
    "def which_vocab(index):\n",
    "    if index < 24200:\n",
    "        return 0, 24200\n",
    "    elif index < 48400:\n",
    "        return 1, 24200\n",
    "    elif index < 51290:\n",
    "        return 2, 2890\n",
    "    elif index < 75490:\n",
    "        return 3, 24200\n",
    "    elif index < 99690:\n",
    "        return 4, 24200\n",
    "    elif index < 123890:\n",
    "        return 5, 24200\n",
    "    elif index < 131159:\n",
    "        return 6, 7269\n",
    "    elif index < 155359:\n",
    "        return 7, 24200\n",
    "    elif index < 179559:\n",
    "        return 8, 24200\n",
    "    elif index < 203759:\n",
    "        return 9, 24200\n",
    "    elif index < 227959:\n",
    "        return 10, 24200\n",
    "    elif index < 252159:\n",
    "        return 11, 24200\n",
    "    elif index < 258227:\n",
    "        return 12, 6068\n",
    "    elif index < 282427:\n",
    "        return 13, 24200\n",
    "    elif index < 306627:\n",
    "        return 14, 24200\n",
    "    elif index < 310023:\n",
    "        return 15, 3396\n",
    "    elif index < 334223:\n",
    "        return 16, 24200\n",
    "    elif index < 358423:\n",
    "        return 17, 24200\n",
    "    elif index < 382623:\n",
    "        return 18, 24200\n",
    "    elif index < 406823:\n",
    "        return 19, 24200\n",
    "    elif index < 418928:\n",
    "        return 20, 12105\n",
    "    elif index < 430425:\n",
    "        return 21, 11497\n",
    "    elif index < 446988:\n",
    "        return 22, 16563\n",
    "    elif index < 452037:\n",
    "        return 23, 5049\n",
    "    elif index < 456571:\n",
    "        return 24, 4534"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab():\n",
    "    with open('vocab.csv') as file:\n",
    "        reader = csv.reader(file)\n",
    "        vocab = list(reader)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_vocab(train_array):\n",
    "    vocab_train_array = []\n",
    "    count = 1\n",
    "    vocab_word = 0\n",
    "    temp_train=[]\n",
    "    for index,training_line in enumerate(train_array):\n",
    "        new_vocab_word, vocab_count = which_vocab(index)\n",
    "        if new_vocab_word != vocab_word:\n",
    "            vocab_train_array.append(temp_train)\n",
    "            temp_train = []\n",
    "            vocab_word=new_vocab_word\n",
    "        temp_train.append(training_line)\n",
    "    vocab_train_array.append(temp_train)\n",
    "    return vocab_train_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for correct split\n",
    "split_vocab_data = split_by_vocab(new_train_vec)\n",
    "for x,array in enumerate(split_vocab_data):\n",
    "    print(vocab[x], len(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_to_labeled(single_vocab_array, vocab, N):\n",
    "    labeled_train = []\n",
    "    for index,training_line in enumerate(single_vocab_array):\n",
    "        label = -1\n",
    "        target_word = -1\n",
    "        if vocab[0] in training_line:\n",
    "            label = 1\n",
    "            target_word = training_line.index(vocab[0])\n",
    "        elif vocab[1] in training_line:\n",
    "            label = 0\n",
    "            target_word = training_line.index(vocab[1])\n",
    "            \n",
    "        if label==-1 or target_word ==-1:\n",
    "            print('error, desired vocab not found')\n",
    "            return 0\n",
    "        \n",
    "        # set number of previous and following words to capture\n",
    "        max_previous = target_word - math.ceil(N/2)\n",
    "        max_forward = target_word + math.floor(N/2)\n",
    "        ngram = [training_line[x] for x in range(max_previous,max_forward) if x != target_word]\n",
    "        ngram.append(label)\n",
    "        labeled_train.append(ngram)\n",
    "    \n",
    "    return labeled_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = train_to_labeled(split_vocab_data[0],vocab[0],7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labeled_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_ngram(labeled_data, word2vec_model):\n",
    "    vectorized_list=[]\n",
    "    labels=[]\n",
    "    for line in labeled_data:\n",
    "        label = line[-1]\n",
    "        vectorized_data = []\n",
    "        for x in line[0:-1]:\n",
    "            if x in word2vec_model.wv:\n",
    "                vectorized_data.extend(word2vec_model.wv[x])\n",
    "            else:\n",
    "                vectorized_data.extend(np.zeros(model.trainables.layer1_size))\n",
    "#         vectorized_data = [word2vec_model[x] for x in line[0:-1] if x in word2vec_model.wv]\n",
    "#         vectorized_data = np.matrix(vectorized_data)\n",
    "        #vectorized_data.append(label)\n",
    "        vectorized_list.append(vectorized_data)\n",
    "        labels.append(label)\n",
    "    return (vectorized_list,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = vectorize_ngram(labeled_data,new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[1],y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X),len(X_train),len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(C=1.0,random_state=0, tol=1e-5)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "log_loss(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_loss(y_test,y_pred))\n",
    "print (y_test[0],y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "svm_reg = LinearSVR(random_state=0, max_iter=50)\n",
    "svm_reg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_reg.predict_proba(X_test_scaled)\n",
    "log_loss(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1, gamma=\"auto\")\n",
    "svm_poly_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_poly_reg.predict(X_test)\n",
    "log_loss(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all(split_vocab_data,model):\n",
    "    y_predictions = []\n",
    "    y_tests = []\n",
    "    models=[]\n",
    "    for index,vocab_example in enumerate(split_vocab_data):\n",
    "        labeled_data = train_to_labeled(vocab_example,vocab[index],5)\n",
    "        X,y = vectorize_ngram(labeled_data,model)\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        clf = LogisticRegression(C=0.0005,random_state=0, solver='lbfgs',max_iter=2000).fit(X_train, y_train)\n",
    "        y_pred=clf.predict_proba(X_test)\n",
    "        models.append(clf)\n",
    "\n",
    "        print(log_loss(y_test,y_pred))\n",
    "        y_predictions.extend(y_pred)\n",
    "        y_tests.extend(y_test)\n",
    "    print(log_loss(y_tests,y_predictions))\n",
    "    return (y_tests,y_predictions,models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_tests,y_pred,models=train_all(split_vocab_data,new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_tests))\n",
    "print(y_pred[91000:91300])\n",
    "log_loss(y_tests,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDRegressor(epsilon=5,max_iter=1000, tol=1e-3, random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "log_loss(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs').fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_test,predict_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "clf = LogisticRegressionCV(cv=5, random_state=0, multi_class='multinomial').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clf.predict_proba(X_test)\n",
    "y_pred[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_test,predict_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=prepare_vec_data('test.txt',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vocab_preprocess(vec_train,set_vocab,vocab_pairs):\n",
    "    new_train_vec=[]\n",
    "    for line in vec_train:\n",
    "        new_line=[]\n",
    "        for word in line:\n",
    "            if word in set_vocab or word in vocab_pairs:\n",
    "                new_line.append(word)\n",
    "            else:\n",
    "                new_line.append('<UNK>')\n",
    "        new_train_vec.append(new_line)\n",
    "    return new_train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_test_data= test_vocab_preprocess(test_data,set_vocab,vocab_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.train(test_data,total_examples=model.corpus_count, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_pairs = ['{'+ x[0]+'|'+x[1]+'}' for x in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_to_ngram(test_array,vocab_pairs,N):\n",
    "    ngram_test = []\n",
    "    target_word = -1\n",
    "    vocab_index = -1\n",
    "    for test_example in test_array:\n",
    "        for x in vocab_pairs:\n",
    "            if x in test_example:\n",
    "                target_word=test_example.index(x)\n",
    "                vocab_index = vocab_pairs.index(x)\n",
    "                break\n",
    "\n",
    "        if target_word ==-1:\n",
    "                print('error, desired vocab not found')\n",
    "                return 0\n",
    "\n",
    "        # set number of previous and following words to capture\n",
    "        max_previous = target_word - math.ceil(N/2)\n",
    "        max_forward = target_word + math.floor(N/2)\n",
    "        ngram = [test_example[x] for x in range(max_previous,max_forward) if x != target_word]\n",
    "        ngram.append(vocab_index)\n",
    "        ngram_test.append(ngram)\n",
    "    \n",
    "    return ngram_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_test = test_to_ngram(test_data,vocab_pairs,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_full,vocab_indices = vectorize_ngram(ngram_test,new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test_full[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_full(X_test_full,vocab_indices,models):\n",
    "    results = ['Id,Expected']\n",
    "    for index,line in enumerate(X_test_full):\n",
    "        results.append(str(index+1) + \",\" + str(models[vocab_indices[index]].predict_proba(np.reshape(line, (1,-1)))[0][0]))\n",
    "    return results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_results = predict_full(X_test_full,vocab_indices,models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_results[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(filename, results):\n",
    "    out_file = open(filename,'w')\n",
    "    for line in results:\n",
    "        out_file.write(line+'\\n')\n",
    "    out_file.close()\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file('linreg_with_word2vec_unk_c0001.csv',y_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_comp",
   "language": "python",
   "name": "ml_comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
