{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import unicodedata as ud\n",
    "import csv\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import training data for vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by converting the given sentences to an array of word arrays, with an appropriate number of pre and post-string characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_split_sentences(filename, N):\n",
    "    pre_string = \"<s>\"\n",
    "    post_string = \"</s>\"\n",
    "    train_dat = []\n",
    "        \n",
    "    with open(filename, encoding=\"utf8\") as file:\n",
    "        for line in file:\n",
    "            line = ud.normalize(\"NFC\",line)\n",
    "            line = re.sub('[,.?\"“”]','',line)\n",
    "            line = re.sub('\\s+',' ',line)\n",
    "            line = line.lower()\n",
    "            split_line = line.strip().split()\n",
    "            for gram in range(1,N):\n",
    "                split_line.insert(0,pre_string)\n",
    "                split_line.append(post_string)\n",
    "            train_dat.append(split_line)\n",
    "    return train_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split_train= file_to_split_sentences('train.txt',5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the word2Vec program to find the 10,000 most common words and store this as 'set_vocab' \n",
    "The length of the vocab is checked to ensure it is close to 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e961851a9fbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_final_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mset_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'split_train' is not defined"
     ]
    }
   ],
   "source": [
    "model=Word2Vec(split_train, size = 200, max_final_vocab=10000)\n",
    "set_vocab = model.wv.vocab\n",
    "print(len(set_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to modify the training array to replace unkown words with the \"UNK\" character and then retrain the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_preprocess(split_file,set_vocab,vocab_pairs=[]):\n",
    "    new_train_vec=[]\n",
    "    for line in split_file:\n",
    "        new_line=[]\n",
    "        for word in line:\n",
    "            if word in set_vocab or word in vocab_pairs:\n",
    "                new_line.append(word)\n",
    "            else:\n",
    "                new_line.append('<UNK>')\n",
    "        new_train_vec.append(new_line)\n",
    "    return new_train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-349a3d9f7069>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_train_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mset_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#Uncomment line below to check for proper tokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnew_train_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_train_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#verify properties of the new model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'split_train' is not defined"
     ]
    }
   ],
   "source": [
    "new_train_vec = vocab_preprocess(split_train,set_vocab)\n",
    "#Uncomment line below to check for proper tokenization\n",
    "new_train_vec[0]\n",
    "new_model=Word2Vec(new_train_vec, size = 200, min_count=1)\n",
    "#verify properties of the new model\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data by vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to determine which vocab word an index is in the training data\n",
    "def which_vocab(index):\n",
    "    if index < 24200:\n",
    "        return 0, 24200\n",
    "    elif index < 48400:\n",
    "        return 1, 24200\n",
    "    elif index < 51290:\n",
    "        return 2, 2890\n",
    "    elif index < 75490:\n",
    "        return 3, 24200\n",
    "    elif index < 99690:\n",
    "        return 4, 24200\n",
    "    elif index < 123890:\n",
    "        return 5, 24200\n",
    "    elif index < 131159:\n",
    "        return 6, 7269\n",
    "    elif index < 155359:\n",
    "        return 7, 24200\n",
    "    elif index < 179559:\n",
    "        return 8, 24200\n",
    "    elif index < 203759:\n",
    "        return 9, 24200\n",
    "    elif index < 227959:\n",
    "        return 10, 24200\n",
    "    elif index < 252159:\n",
    "        return 11, 24200\n",
    "    elif index < 258227:\n",
    "        return 12, 6068\n",
    "    elif index < 282427:\n",
    "        return 13, 24200\n",
    "    elif index < 306627:\n",
    "        return 14, 24200\n",
    "    elif index < 310023:\n",
    "        return 15, 3396\n",
    "    elif index < 334223:\n",
    "        return 16, 24200\n",
    "    elif index < 358423:\n",
    "        return 17, 24200\n",
    "    elif index < 382623:\n",
    "        return 18, 24200\n",
    "    elif index < 406823:\n",
    "        return 19, 24200\n",
    "    elif index < 418928:\n",
    "        return 20, 12105\n",
    "    elif index < 430425:\n",
    "        return 21, 11497\n",
    "    elif index < 446988:\n",
    "        return 22, 16563\n",
    "    elif index < 452037:\n",
    "        return 23, 5049\n",
    "    elif index < 456571:\n",
    "        return 24, 4534"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab():\n",
    "    with open('vocab.csv') as file:\n",
    "        reader = csv.reader(file)\n",
    "        vocab = list(reader)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then split the data by the possible vocab words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_vocab(train_array):\n",
    "    vocab_train_array = []\n",
    "    count = 1\n",
    "    vocab_word = 0\n",
    "    temp_train=[]\n",
    "    for index,training_line in enumerate(train_array):\n",
    "        new_vocab_word, vocab_count = which_vocab(index)\n",
    "        if new_vocab_word != vocab_word:\n",
    "            vocab_train_array.append(temp_train)\n",
    "            temp_train = []\n",
    "            vocab_word=new_vocab_word\n",
    "        temp_train.append(training_line)\n",
    "    vocab_train_array.append(temp_train)\n",
    "    return vocab_train_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-12f56a69b2f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msplit_vocab_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_by_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#uncomment below to check for correct split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_vocab_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'split_train' is not defined"
     ]
    }
   ],
   "source": [
    "split_vocab_data = split_by_vocab(split_train)\n",
    "#uncomment below to check for correct split\n",
    "for x,array in enumerate(split_vocab_data):\n",
    "    print(vocab[x], len(array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a function to check if a word has lenition or eclipsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_mutations(word):\n",
    "    lenition =0\n",
    "    eclipsis = 0\n",
    "    eclipsis_list=('mb','gc','nd','ng','bhf','bp','dt','n-a','n-á','n-e','n-é','n-i','n-í','n-o','n-ó','n-u','n-ú','n-y')\n",
    "            \n",
    "    if word.startswith(eclipsis_list):\n",
    "        eclipsis=1\n",
    "    elif len(word)>2 and word[1]=='h':\n",
    "        lenition=1\n",
    "    \n",
    "    return (lenition,eclipsis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to create labelled examples from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_to_labeled(single_vocab_array, vocab, N, mutations=True):\n",
    "    labeled_train = []\n",
    "    for index,training_line in enumerate(single_vocab_array):\n",
    "        label = -1\n",
    "        target_word = -1\n",
    "        if vocab[0] in training_line:\n",
    "            label = 1\n",
    "            target_word = training_line.index(vocab[0])\n",
    "        elif vocab[1] in training_line:\n",
    "            label = 0\n",
    "            target_word = training_line.index(vocab[1])\n",
    "            \n",
    "        if label==-1 or target_word ==-1:\n",
    "            print('error, desired vocab not found')\n",
    "            return 0\n",
    "        \n",
    "        # set number of previous and following words to capture\n",
    "        max_previous = target_word - math.ceil(N/2)\n",
    "        max_forward = target_word + math.floor(N/2)\n",
    "        ngram = [training_line[x] for x in range(max_previous,max_forward) if x != target_word]\n",
    "        if mutations:\n",
    "            lenition,eclipsis=check_mutations(ngram[math.ceil(len(ngram)/2)])\n",
    "            ngram.append(lenition)\n",
    "            ngram.append(eclipsis)\n",
    "        ngram.append(label)\n",
    "        labeled_train.append(ngram)\n",
    "    \n",
    "    return labeled_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_vocab_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-97b965afc018>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabeled_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_to_labeled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_vocab_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#uncomment to check for proper labeled data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabeled_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'split_vocab_data' is not defined"
     ]
    }
   ],
   "source": [
    "labeled_data = train_to_labeled(split_vocab_data[0],vocab[0],5)\n",
    "#uncomment to check for proper labeled data\n",
    "labeled_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the labelled data needs to be vectorized. This returns a vector of vector_size x (N_gram-1) dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_ngram(labeled_data, word2vec_model, set_vocab,vocab_pairs=[]):\n",
    "    vectorized_list=[]\n",
    "    labels=[]\n",
    "    for line in labeled_data:\n",
    "        label = line[-1]\n",
    "        eclipsis=line[-2]\n",
    "        lenition=line[-3]\n",
    "        vectorized_data = []\n",
    "        for x in line[0:-3]:\n",
    "            if x not in word2vec_model.wv:\n",
    "                x = '<UNK>'\n",
    "            if x in word2vec_model.wv or x in vocab_pairs:\n",
    "                vectorized_data.extend(word2vec_model.wv[x])\n",
    "            else:\n",
    "                vectorized_data.extend(np.zeros(model.trainables.layer1_size))\n",
    "        vectorized_data.append(lenition)\n",
    "        vectorized_data.append(eclipsis)\n",
    "            \n",
    "        vectorized_list.append(vectorized_data)\n",
    "        labels.append(label)\n",
    "    return (vectorized_list,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labeled_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ba57ce116b0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize_ngram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mset_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'labeled_data' is not defined"
     ]
    }
   ],
   "source": [
    "X,y = vectorize_ngram(labeled_data,new_model,set_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-8661e3775007>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "len(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_rnn_ngram(labeled_data, word2vec_model, set_vocab,vocab_pairs=[]):\n",
    "    vectorized_list=[]\n",
    "    labels=[]\n",
    "    for line in labeled_data:\n",
    "        label = line[-1]\n",
    "        vectorized_data = []\n",
    "        for x in line[0:-1]:\n",
    "            if x not in word2vec_model.wv:\n",
    "                x = '<UNK>'\n",
    "            if x in word2vec_model.wv or x in vocab_pairs:\n",
    "                vectorized_data.append(word2vec_model.wv[x])\n",
    "            else:\n",
    "                vectorized_data.append(np.zeros(model.trainables.layer1_size))      \n",
    "        vectorized_list.append(vectorized_data)\n",
    "        labels.append(label)\n",
    "    return (vectorized_list,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is properly vectorized, we want to split into a training and development set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-7c99675faa22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#uncomment to check for proper lengths of test and training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "#uncomment to check for proper lengths of test and training set\n",
    "print(len(X),len(X_train),len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the above preprocessing steps into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_prep(filename,N,vec_size,max_vocab):\n",
    "    split_train=file_to_split_sentences(filename,N)\n",
    "    model=Word2Vec(split_train,size=vec_size,max_final_vocab=max_vocab)\n",
    "    set_vocab=model.wv.vocab\n",
    "    new_train_vec=vocab_preprocess(split_train,set_vocab)\n",
    "    new_model=Word2Vec(new_train_vec,size=vec_size,max_final_vocab=max_vocab)\n",
    "    vocab = create_vocab()\n",
    "    split_vocab_data = split_by_vocab(split_train)\n",
    "    X_train_list=[]\n",
    "    X_dev_list=[]\n",
    "    y_train_list=[]\n",
    "    y_dev_list=[]\n",
    "    for index,vocab_data in enumerate(split_vocab_data):\n",
    "        labeled_data=train_to_labeled(vocab_data,vocab[index],N)\n",
    "        X,y = vectorize_ngram(labeled_data,new_model,set_vocab)\n",
    "        X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "        X_train_list.append(X_train)\n",
    "        X_dev_list.append(X_dev)\n",
    "        y_train_list.append(y_train)\n",
    "        y_dev_list.append(y_dev)\n",
    "    return (X_train_list,X_dev_list,y_train_list,y_dev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list,X_dev_list,y_train_list,y_dev_list = train_data_prep('train.txt',5,200,10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a new pipeline for training data to be fed to an RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn_data_prep(filename,N,vec_size,max_vocab):\n",
    "    split_train=file_to_split_sentences(filename,N)\n",
    "    model=Word2Vec(split_train,size=vec_size,max_final_vocab=max_vocab)\n",
    "    set_vocab=model.wv.vocab\n",
    "    new_train_vec=vocab_preprocess(split_train,set_vocab)\n",
    "    new_model=Word2Vec(new_train_vec,size=vec_size,max_final_vocab=max_vocab)\n",
    "    vocab = create_vocab()\n",
    "    split_vocab_data = split_by_vocab(split_train)\n",
    "    X_train_list=[]\n",
    "    X_dev_list=[]\n",
    "    y_train_list=[]\n",
    "    y_dev_list=[]\n",
    "    for index,vocab_data in enumerate(split_vocab_data):\n",
    "        labeled_data=train_to_labeled(vocab_data,vocab[index],N,mutations=False)\n",
    "        X,y = vectorize_rnn_ngram(labeled_data,new_model,set_vocab)\n",
    "        X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "        X_train_list.append(X_train)\n",
    "        X_dev_list.append(X_dev)\n",
    "        y_train_list.append(y_train)\n",
    "        y_dev_list.append(y_dev)\n",
    "    return (X_train_list,X_dev_list,y_train_list,y_dev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rnn,X_dev_rnn,y_train_rnn,y_dev_rnn = train_rnn_data_prep('train.txt',5,200,10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting to apply ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have vectorized data, we can start applying ML algorithms using sklearn. Note, we need to train 25 individual classifiers- one for each of the vocab words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to set X,y train and dev\n",
    "def get_single_vocab(X_train_list,X_dev_list,y_train_list,y_dev_list,index=0):\n",
    "    return (X_train_list[index],X_dev_list[index],y_train_list[index],y_dev_list[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_dev,y_train,y_dev=get_single_vocab(X_train_list,X_dev_list,y_train_list,y_dev_list,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets start with a logistic regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2948328172350819\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(C=0.001,random_state=0, solver='lbfgs')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict_proba(X_dev)\n",
    "#evaluate performance using log_loss\n",
    "print(log_loss(y_dev,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If evaluation on one vocab word looks good, lets try it on all vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to test all of the vocab given a model\n",
    "def test_all_vocab(classifier):\n",
    "    import statistics\n",
    "    y_dev_losses=[]\n",
    "    y_train_losses=[]\n",
    "    for x in range(0,25):\n",
    "        X_train,X_dev,y_train,y_dev=get_single_vocab(X_train_list,X_dev_list,y_train_list,y_dev_list,x)\n",
    "        classifier.fit(X_train,y_train)\n",
    "        y_pred = classifier.predict_proba(X_dev)\n",
    "        y_train_pred = classifier.predict_proba(X_train)\n",
    "        #evaluate performance using log_loss\n",
    "        train_loss=log_loss(y_train,y_train_pred)\n",
    "        dev_loss=log_loss(y_dev,y_pred)\n",
    "        y_dev_losses.append(dev_loss)\n",
    "        y_train_losses.append(train_loss)\n",
    "        \n",
    "        print('Train loss: ',train_loss,' Val loss: ',dev_loss)\n",
    "    print('Train loss: ',statistics.mean(y_train_losses),' Val loss: ',statistics.mean(y_dev_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.05926530865696404  Val loss:  0.06897391022344879\n",
      "Train loss:  0.0511431767767427  Val loss:  0.047403833913009764\n",
      "Train loss:  0.25843892366018606  Val loss:  0.2948328172350819\n",
      "Train loss:  0.0902894573080811  Val loss:  0.08471233320408933\n",
      "Train loss:  0.034367420244572434  Val loss:  0.03928544736386444\n",
      "Train loss:  0.0668311699897007  Val loss:  0.06250380816839228\n",
      "Train loss:  0.2847768720033905  Val loss:  0.32178525529667645\n",
      "Train loss:  0.13820219544895246  Val loss:  0.1413046710539599\n",
      "Train loss:  0.08477643321962271  Val loss:  0.09306980339354773\n",
      "Train loss:  0.030256108037642077  Val loss:  0.036460564256765145\n",
      "Train loss:  0.2657636969747682  Val loss:  0.2866241989376655\n",
      "Train loss:  0.08108906425617594  Val loss:  0.09222721743653677\n",
      "Train loss:  0.08740586806439218  Val loss:  0.10920184497923477\n",
      "Train loss:  0.05081754612564946  Val loss:  0.04973762828894898\n",
      "Train loss:  0.20932555006469156  Val loss:  0.21056522584316295\n",
      "Train loss:  0.40607723866542467  Val loss:  0.4655420710703433\n",
      "Train loss:  0.14139176394629446  Val loss:  0.1581543652915326\n",
      "Train loss:  0.0523554186975805  Val loss:  0.06333785020676795\n",
      "Train loss:  0.07312481190929972  Val loss:  0.06698336574297951\n",
      "Train loss:  0.26761618695571626  Val loss:  0.2846485145454839\n",
      "Train loss:  0.14560817813933283  Val loss:  0.1589580090735119\n",
      "Train loss:  0.2020060947255958  Val loss:  0.21441144796795752\n",
      "Train loss:  0.08916774697283755  Val loss:  0.08682134022480928\n",
      "Train loss:  0.15769804869684456  Val loss:  0.15163174077711958\n",
      "Train loss:  0.08127922679145413  Val loss:  0.08780226165558935\n",
      "Train loss:  0.1363629402532765  Val loss:  0.1470791810460192\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.001,random_state=0, solver='lbfgs')\n",
    "test_all_vocab(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also take a peek at the predictions the model is making.\n",
    "If these values are too close to 1 or 0, it may indicate overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression looks pretty good, so lets see how logistic regression with CV performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_dev,y_train,y_dev=get_single_vocab(X_train_list,X_dev_list,y_train_list,y_dev_list,2)\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "clf = LogisticRegressionCV(Cs=np.arange(0.001,0.01,0.001),cv=5,random_state=0, solver='lbfgs',max_iter=200, scoring='neg_log_loss')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict_proba(X_dev)\n",
    "#evaluate performance using log_loss\n",
    "print(log_loss(y_dev,y_pred))\n",
    "print(clf.C_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this looks reasonable, so let's try it with all the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegressionCV(Cs=[0.00001,0.0001,0.001],cv=5,random_state=0, solver='lbfgs',max_iter=200)\n",
    "for x in range(0,25):\n",
    "    X_train,X_dev,y_train,y_dev=get_single_vocab(X_train_list,X_dev_list,y_train_list,y_dev_list,x)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred=clf.predict_proba(X_dev)\n",
    "    #evaluate performance using log_loss\n",
    "    print(log_loss(y_dev,y_pred),clf.C_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 0.0642826738215069 | 0.00599484\n",
    "* 0.04229903577299008 | 0.00599484\n",
    "* 0.249076453782974 | 0.00599484\n",
    "* 0.08190084583878704 | 0.00599484\n",
    "* 0.03096742615321697 | 0.04641589\n",
    "* 0.044940145400178114 | 0.04641589\n",
    "* 0.2887409855573007 | 0.04641589\n",
    "* 0.12843203781524062 | 0.00599484\n",
    "* 0.08706723982279557 | 0.00599484\n",
    "* 0.02103945686975264 | 0.04641589\n",
    "* 0.28598412543703405 | 0.00599484\n",
    "* 0.07827273100206689 | 0.00599484\n",
    "* 0.10526719899321839 | 0.00599484\n",
    "* 0.03778233770808103 | 0.04641589\n",
    "* 0.20574933511333274 | 0.00599484\n",
    "* 0.4577377917069274 | 0.0464158\n",
    "* 0.1494325625192918 | 0.04641589\n",
    "* 0.06065848401236185 | 0.00599484\n",
    "* 0.04519150781077979 | 0.04641589\n",
    "* 0.28603926814517255 | 0.00599484\n",
    "* 0.14627178136852173 | 0.00599484\n",
    "* 0.19339296575800236 | 0.00599484\n",
    "* 0.08455027218569924 | 0.00599484\n",
    "* 0.11692524801390788 | 0.00599484\n",
    "* 0.09163245328102149 | 0.00077426\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like 0.006 is a reasonable choice for C, and we can stick with just plain linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets try a naive_bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_dev,y_train,y_dev=get_single_vocab(X_train_list,X_dev_list,y_train_list,y_dev_list,2)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict_proba(X_dev)\n",
    "Y_train_pred=gnb.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5545733030767206\n"
     ]
    }
   ],
   "source": [
    "print(log_loss(y_dev,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4572312020490226\n"
     ]
    }
   ],
   "source": [
    "print(log_loss(y_train,Y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is just a poor fit for this data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets give a random forest a try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_dev,y_train,y_dev=get_single_vocab(X_train_list,X_dev_list,y_train_list,y_dev_list,2)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=1000, max_depth=4,random_state=0)\n",
    "rfc.fit(X_train,y_train)\n",
    "y_train_pred=rfc.predict_proba(X_train)\n",
    "y_pred = rfc.predict_proba(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3395842600818164\n",
      "0.36933733956990483\n"
     ]
    }
   ],
   "source": [
    "print(log_loss(y_train,y_train_pred))\n",
    "print(log_loss(y_dev,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, this seems like a promising option, so lets try on all vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.07729325174759674  Val loss:  0.0889130517898909\n",
      "Train loss:  0.057677241105136993  Val loss:  0.054410865651567754\n",
      "Train loss:  0.34123919444475176  Val loss:  0.3682932570420379\n",
      "Train loss:  0.10067228716088573  Val loss:  0.09744645573457894\n",
      "Train loss:  0.04133242650791395  Val loss:  0.045515030557748984\n",
      "Train loss:  0.09788706554453475  Val loss:  0.08896222534068549\n",
      "Train loss:  0.3682677275683832  Val loss:  0.3852213233709328\n",
      "Train loss:  0.26738266335208793  Val loss:  0.26324465945315884\n",
      "Train loss:  0.10842978114980512  Val loss:  0.11633480548777697\n",
      "Train loss:  0.052866802168460035  Val loss:  0.06298578553478376\n",
      "Train loss:  0.2991935974151097  Val loss:  0.3104345083179092\n",
      "Train loss:  0.12170242873848708  Val loss:  0.12709269918970287\n",
      "Train loss:  0.08417111710599486  Val loss:  0.11114224108008605\n",
      "Train loss:  0.07257755760037701  Val loss:  0.07281442802186473\n",
      "Train loss:  0.29224022410996536  Val loss:  0.2880289634083919\n",
      "Train loss:  0.4902269943335784  Val loss:  0.5385294448414284\n",
      "Train loss:  0.1863763859086286  Val loss:  0.20031514454158214\n",
      "Train loss:  0.06269381997492982  Val loss:  0.0773322585992733\n",
      "Train loss:  0.12338247113848458  Val loss:  0.11261022276100306\n",
      "Train loss:  0.2763460975458094  Val loss:  0.28592417502998013\n",
      "Train loss:  0.16855346498328977  Val loss:  0.18040987952637935\n",
      "Train loss:  0.2738269256536044  Val loss:  0.27879183064711727\n",
      "Train loss:  0.09278479514847619  Val loss:  0.0864640542016399\n",
      "Train loss:  0.20012861685186886  Val loss:  0.19388824389142556\n",
      "Train loss:  0.06594554165022966  Val loss:  0.08599254530235274\n",
      "Train loss:  0.17292793915633559  Val loss:  0.18084392397293195\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=4,random_state=0)\n",
    "test_all_vocab(rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this looks pretty good with minimal effort, so it may be good to explore further. Logistic regression seems to still have it beat though"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try out an extra trees classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_dev,y_train,y_dev=get_single_vocab(X_train_list,X_dev_list,y_train_list,y_dev_list,2)\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "etc = ExtraTreesClassifier(n_estimators=1000, max_depth=2,random_state=0)\n",
    "etc.fit(X_train,y_train)\n",
    "y_train_pred=etc.predict_proba(X_train)\n",
    "y_pred = etc.predict_proba(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.573918133441894\n",
      "0.5807373699481648\n"
     ]
    }
   ],
   "source": [
    "print(log_loss(y_train,y_train_pred))\n",
    "print(log_loss(y_dev,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, let's give this a go also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etc = RandomForestClassifier(n_estimators=100, max_depth=4,random_state=0)\n",
    "test_all_vocab(etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also has relatively good performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets give an SVM a try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_dev,y_train,y_dev=get_single_vocab(X_train_list,X_dev_list,y_train_list,y_dev_list,2)\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#scaler=StandardScaler()\n",
    "svm = SVC(C=0.001,gamma='auto',probability=True,random_state=0)\n",
    "#svm.fit(scaler.fit_transform(X_train),y_train)\n",
    "svm.fit(X_train,y_train)\n",
    "y_train_pred=svm.predict_proba(X_train)\n",
    "y_pred = svm.predict_proba(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3754262517132494\n",
      "0.4201871857408247\n"
     ]
    }
   ],
   "source": [
    "print(log_loss(y_train,y_train_pred))\n",
    "print(log_loss(y_dev,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, worth a shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# svm = SVC(C=0.0001,gamma='auto',probability=True,random_state=0)\n",
    "# test_all_vocab(svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, lets try a very basic neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.reshape(X_train[0],(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_dev,y_train,y_dev=get_single_vocab(X_train_list,X_dev_list,y_train_list,y_dev_list,2)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Dense(300, input_shape = (len(X_train[0]),),activation='relu'))\n",
    "keras_model.add(Dropout(0.9)) \n",
    "keras_model.add(Dense(300, activation='relu'))\n",
    "keras_model.add(Dense(1, activation='sigmoid'))\n",
    "keras_model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2312 samples, validate on 578 samples\n",
      "Epoch 1/10\n",
      "2312/2312 [==============================] - 2s 803us/step - loss: 0.6830 - acc: 0.7020 - val_loss: 0.3093 - val_acc: 0.8789\n",
      "Epoch 2/10\n",
      "2312/2312 [==============================] - 0s 112us/step - loss: 0.5083 - acc: 0.7971 - val_loss: 0.2885 - val_acc: 0.8893\n",
      "Epoch 3/10\n",
      "2312/2312 [==============================] - 0s 113us/step - loss: 0.4108 - acc: 0.8426 - val_loss: 0.2766 - val_acc: 0.9135\n",
      "Epoch 4/10\n",
      "2312/2312 [==============================] - 0s 112us/step - loss: 0.4064 - acc: 0.8681 - val_loss: 0.2693 - val_acc: 0.9066\n",
      "Epoch 5/10\n",
      "2312/2312 [==============================] - 0s 114us/step - loss: 0.3640 - acc: 0.8789 - val_loss: 0.2506 - val_acc: 0.9100\n",
      "Epoch 6/10\n",
      "2312/2312 [==============================] - 0s 111us/step - loss: 0.3144 - acc: 0.8945 - val_loss: 0.2603 - val_acc: 0.9170\n",
      "Epoch 7/10\n",
      "2312/2312 [==============================] - 0s 118us/step - loss: 0.3013 - acc: 0.8966 - val_loss: 0.2584 - val_acc: 0.9152\n",
      "Epoch 8/10\n",
      "2312/2312 [==============================] - 0s 115us/step - loss: 0.2865 - acc: 0.9018 - val_loss: 0.2587 - val_acc: 0.9100\n",
      "Epoch 9/10\n",
      "2312/2312 [==============================] - 0s 112us/step - loss: 0.2392 - acc: 0.9187 - val_loss: 0.2472 - val_acc: 0.9204\n",
      "Epoch 10/10\n",
      "2312/2312 [==============================] - 0s 113us/step - loss: 0.2492 - acc: 0.9131 - val_loss: 0.2486 - val_acc: 0.9239\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f59431b1b00>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_model.fit(np.array(X_train), np.array(y_train), epochs=10,validation_data=(np.array(X_dev),np.array(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2312/2312 [==============================] - 0s 28us/step\n",
      "578/578 [==============================] - 0s 26us/step\n"
     ]
    }
   ],
   "source": [
    "train_scores = scores = keras_model.evaluate(np.array(X_train),np.array(y_train))\n",
    "dev_scores = keras_model.evaluate(np.array(X_dev),np.array(y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18341195347451422, 0.9662629757785467] [0.32745241108237666, 0.9100346020761245]\n"
     ]
    }
   ],
   "source": [
    "print(train_scores,dev_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all of the rnn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rnn,X_dev_rnn,y_train_rnn,y_dev_rnn = train_rnn_data_prep('train.txt',5,200,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2312, 4, 200)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_dev,y_train,y_dev=get_single_vocab(X_train_rnn,X_dev_rnn,y_train_rnn,y_dev_rnn,2)\n",
    "print(np.array(X_train).shape)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Embedding, Flatten\n",
    "rnn_model = Sequential()\n",
    "#rnn_model.add(Dense(128, input_shape = (len(X_train[0]),),activation='relu'))\n",
    "rnn_model.add(LSTM(100,input_shape=(len(X_train[1]),len(X_train[0][0])),return_sequences=True,dropout=0.7,recurrent_dropout=0.7))\n",
    "rnn_model.add(LSTM(50,return_sequences=True,dropout=0.7,recurrent_dropout=0.7))\n",
    "rnn_model.add(LSTM(10,dropout=0.7,recurrent_dropout=0.7))\n",
    "#rnn_model.add(Flatten())\n",
    "rnn_model.add(Dense(1, activation='sigmoid'))\n",
    "rnn_model.compile(loss='binary_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2312 samples, validate on 578 samples\n",
      "Epoch 1/10\n",
      "2312/2312 [==============================] - 8s 4ms/step - loss: 0.6082 - val_loss: 0.4469\n",
      "Epoch 2/10\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.4454 - val_loss: 0.3545\n",
      "Epoch 3/10\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.4036 - val_loss: 0.3199\n",
      "Epoch 4/10\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.3660 - val_loss: 0.3238\n",
      "Epoch 5/10\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.3532 - val_loss: 0.3022\n",
      "Epoch 6/10\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.3392 - val_loss: 0.3050\n",
      "Epoch 7/10\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.3302 - val_loss: 0.3121\n",
      "Epoch 8/10\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.3064 - val_loss: 0.3342\n",
      "Epoch 9/10\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.3086 - val_loss: 0.3090\n",
      "Epoch 10/10\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2916 - val_loss: 0.3200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f595c48eeb8>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.fit(np.array(X_train), np.array(y_train), epochs=10, batch_size=5, validation_data=((np.array(X_dev),np.array(y_dev))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2312, 4, 200)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_dev,y_train,y_dev=get_single_vocab(X_train_rnn,X_dev_rnn,y_train_rnn,y_dev_rnn,2)\n",
    "print(np.array(X_train).shape)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Embedding, Flatten\n",
    "rnn_model = Sequential()\n",
    "#rnn_model.add(Dense(128, input_shape = (len(X_train[0]),),activation='relu'))\n",
    "rnn_model.add(LSTM(30,input_shape=(len(X_train[1]),len(X_train[0][0])),dropout=0.7,recurrent_dropout=0.7))\n",
    "rnn_model.add(Dense(40, activation='tanh'))\n",
    "#rnn_model.add(Flatten())\n",
    "rnn_model.add(Dense(1, activation='sigmoid'))\n",
    "rnn_model.compile(loss='binary_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2312 samples, validate on 578 samples\n",
      "Epoch 1/10\n",
      "2312/2312 [==============================] - 4s 2ms/step - loss: 0.5208 - val_loss: 0.3160\n",
      "Epoch 2/10\n",
      "2312/2312 [==============================] - 2s 985us/step - loss: 0.3767 - val_loss: 0.2930\n",
      "Epoch 3/10\n",
      "2312/2312 [==============================] - 2s 989us/step - loss: 0.3388 - val_loss: 0.2877\n",
      "Epoch 4/10\n",
      "2312/2312 [==============================] - 2s 1ms/step - loss: 0.3317 - val_loss: 0.2860\n",
      "Epoch 5/10\n",
      "2312/2312 [==============================] - 2s 988us/step - loss: 0.2969 - val_loss: 0.2892\n",
      "Epoch 6/10\n",
      "2312/2312 [==============================] - 2s 974us/step - loss: 0.2947 - val_loss: 0.2907\n",
      "Epoch 7/10\n",
      "2312/2312 [==============================] - 2s 978us/step - loss: 0.2723 - val_loss: 0.2981\n",
      "Epoch 8/10\n",
      "2312/2312 [==============================] - 2s 990us/step - loss: 0.2976 - val_loss: 0.2798\n",
      "Epoch 9/10\n",
      "2312/2312 [==============================] - 2s 993us/step - loss: 0.2757 - val_loss: 0.2866\n",
      "Epoch 10/10\n",
      "2312/2312 [==============================] - 2s 981us/step - loss: 0.2658 - val_loss: 0.2802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f59428c60b8>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.fit(np.array(X_train), np.array(y_train), epochs=10, batch_size=5, validation_data=((np.array(X_dev),np.array(y_dev))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2312/2312 [==============================] - 0s 40us/step\n",
      "578/578 [==============================] - 0s 50us/step\n"
     ]
    }
   ],
   "source": [
    "train_scores = scores = rnn_model.evaluate(np.array(X_train),np.array(y_train))\n",
    "dev_scores = rnn_model.evaluate(np.array(X_dev),np.array(y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16922360038623266 0.28020085830718383\n"
     ]
    }
   ],
   "source": [
    "print(train_scores,dev_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2312, 6, 300)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_dev,y_train,y_dev=get_single_vocab(X_train_rnn,X_dev_rnn,y_train_rnn,y_dev_rnn,2)\n",
    "print(np.array(X_train).shape)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Embedding, Flatten\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(LSTM(8,input_shape=(len(X_train[1]),len(X_train[0][0])),dropout=0.7,recurrent_dropout=0.7,return_sequences=True))\n",
    "rnn_model.add(LSTM(8,dropout=0.7,recurrent_dropout=0.7))\n",
    "rnn_model.add(Dense(1, activation='sigmoid'))\n",
    "rnn_model.compile(loss='binary_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2312 samples, validate on 578 samples\n",
      "Epoch 1/50\n",
      "2312/2312 [==============================] - 18s 8ms/step - loss: 0.6873 - val_loss: 0.6192\n",
      "Epoch 2/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.6171 - val_loss: 0.4147\n",
      "Epoch 3/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.5102 - val_loss: 0.3528\n",
      "Epoch 4/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.4429 - val_loss: 0.3311\n",
      "Epoch 5/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.4173 - val_loss: 0.3138\n",
      "Epoch 6/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.3895 - val_loss: 0.3132\n",
      "Epoch 7/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.3736 - val_loss: 0.3043\n",
      "Epoch 8/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.3445 - val_loss: 0.2982\n",
      "Epoch 9/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.3442 - val_loss: 0.2908\n",
      "Epoch 10/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.3418 - val_loss: 0.2929\n",
      "Epoch 11/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.3221 - val_loss: 0.2911\n",
      "Epoch 12/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.3231 - val_loss: 0.2823\n",
      "Epoch 13/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.3187 - val_loss: 0.2783\n",
      "Epoch 14/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.3118 - val_loss: 0.2786\n",
      "Epoch 15/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.3082 - val_loss: 0.2787\n",
      "Epoch 16/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2990 - val_loss: 0.2809\n",
      "Epoch 17/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2850 - val_loss: 0.2774\n",
      "Epoch 18/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.3022 - val_loss: 0.2870\n",
      "Epoch 19/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2952 - val_loss: 0.2741\n",
      "Epoch 20/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2972 - val_loss: 0.2765\n",
      "Epoch 21/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2993 - val_loss: 0.2803\n",
      "Epoch 22/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2992 - val_loss: 0.2719\n",
      "Epoch 23/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2876 - val_loss: 0.2692\n",
      "Epoch 24/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2913 - val_loss: 0.2688\n",
      "Epoch 25/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2967 - val_loss: 0.2634\n",
      "Epoch 26/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2759 - val_loss: 0.2636\n",
      "Epoch 27/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2978 - val_loss: 0.2602\n",
      "Epoch 28/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2699 - val_loss: 0.2610\n",
      "Epoch 29/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2898 - val_loss: 0.2591\n",
      "Epoch 30/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2983 - val_loss: 0.2618\n",
      "Epoch 31/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2866 - val_loss: 0.2616\n",
      "Epoch 32/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2606 - val_loss: 0.2643\n",
      "Epoch 33/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2943 - val_loss: 0.2596\n",
      "Epoch 34/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2642 - val_loss: 0.2586\n",
      "Epoch 35/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2833 - val_loss: 0.2599\n",
      "Epoch 36/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2710 - val_loss: 0.2570\n",
      "Epoch 37/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2766 - val_loss: 0.2518\n",
      "Epoch 38/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2655 - val_loss: 0.2515\n",
      "Epoch 39/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2614 - val_loss: 0.2524\n",
      "Epoch 40/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2642 - val_loss: 0.2525\n",
      "Epoch 41/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2652 - val_loss: 0.2506\n",
      "Epoch 42/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2750 - val_loss: 0.2452\n",
      "Epoch 43/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2662 - val_loss: 0.2472\n",
      "Epoch 44/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2634 - val_loss: 0.2484\n",
      "Epoch 45/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2763 - val_loss: 0.2478\n",
      "Epoch 46/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2568 - val_loss: 0.2492\n",
      "Epoch 47/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2697 - val_loss: 0.2511\n",
      "Epoch 48/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2779 - val_loss: 0.2471\n",
      "Epoch 49/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2595 - val_loss: 0.2484\n",
      "Epoch 50/50\n",
      "2312/2312 [==============================] - 5s 2ms/step - loss: 0.2644 - val_loss: 0.2449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9cd84a9550>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.fit(np.array(X_train), np.array(y_train), epochs=50, batch_size=5, validation_data=((np.array(X_dev),np.array(y_dev))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run this through all the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn_all(X_train_list,X_dev_list,y_train_list,y_dev_list,vocab):\n",
    "    y_predictions = []\n",
    "    y_tests = []\n",
    "    models=[]\n",
    "    for x in range(0,len(vocab)):\n",
    "        X_train,X_dev,y_train,y_dev=get_single_vocab(X_train_list,X_dev_list,y_train_list,y_dev_list,x)\n",
    "        rnn_model = Sequential()\n",
    "        rnn_model.add(LSTM(8,input_shape=(len(X_train[1]),len(X_train[0][0])),dropout=0.7,recurrent_dropout=0.7,return_sequences=True))\n",
    "        rnn_model.add(LSTM(8,dropout=0.7,recurrent_dropout=0.7))\n",
    "        rnn_model.add(Dense(1, activation='sigmoid'))\n",
    "        rnn_model.compile(loss='binary_crossentropy',optimizer='adam')\n",
    "        rnn.fit(np.array(X_train),np.array(y_train))\n",
    "        models.append(rnn)\n",
    "        y_pred = rnn.predict(np.array(X_dev))\n",
    "        y_predictions.extend(y_pred)\n",
    "        y_tests.extend(y_dev)\n",
    "    return (y_tests,y_predictions,models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_rnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-599548ff1d2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_rnn_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_rnn_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnn_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_rnn_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_rnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_dev_rnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_rnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_dev_rnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_rnn' is not defined"
     ]
    }
   ],
   "source": [
    "y_rnn_test,y_rnn_pred,rnn_models=train_rnn_all(X_train_rnn,X_dev_rnn,y_train_rnn,y_dev_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2716, 6, 300)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_dev,y_train,y_dev=get_single_vocab(X_train_rnn,X_dev_rnn,y_train_rnn,y_dev_rnn,15)\n",
    "print(np.array(X_train).shape)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Embedding, Flatten\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(LSTM(8,input_shape=(len(X_train[1]),len(X_train[0][0])),dropout=0.7,recurrent_dropout=0.7,return_sequences=True))\n",
    "rnn_model.add(LSTM(8,dropout=0.7,recurrent_dropout=0.7))\n",
    "#rnn_model.add(Flatten())\n",
    "rnn_model.add(Dense(1, activation='sigmoid'))\n",
    "rnn_model.compile(loss='binary_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2716 samples, validate on 680 samples\n",
      "Epoch 1/100\n",
      "2716/2716 [==============================] - 15s 6ms/step - loss: 0.6913 - val_loss: 0.6817\n",
      "Epoch 2/100\n",
      "2716/2716 [==============================] - 0s 122us/step - loss: 0.6808 - val_loss: 0.6735\n",
      "Epoch 3/100\n",
      "2716/2716 [==============================] - 0s 124us/step - loss: 0.6756 - val_loss: 0.6666\n",
      "Epoch 4/100\n",
      "2716/2716 [==============================] - 0s 126us/step - loss: 0.6681 - val_loss: 0.6592\n",
      "Epoch 5/100\n",
      "2716/2716 [==============================] - 0s 127us/step - loss: 0.6616 - val_loss: 0.6496\n",
      "Epoch 6/100\n",
      "2716/2716 [==============================] - 0s 127us/step - loss: 0.6571 - val_loss: 0.6361\n",
      "Epoch 7/100\n",
      "2716/2716 [==============================] - 0s 127us/step - loss: 0.6467 - val_loss: 0.6159\n",
      "Epoch 8/100\n",
      "2716/2716 [==============================] - 0s 124us/step - loss: 0.6371 - val_loss: 0.5931\n",
      "Epoch 9/100\n",
      "2716/2716 [==============================] - 0s 125us/step - loss: 0.6230 - val_loss: 0.5721\n",
      "Epoch 10/100\n",
      "2716/2716 [==============================] - 0s 125us/step - loss: 0.6064 - val_loss: 0.5531\n",
      "Epoch 11/100\n",
      "2716/2716 [==============================] - 0s 123us/step - loss: 0.5902 - val_loss: 0.5333\n",
      "Epoch 12/100\n",
      "2716/2716 [==============================] - 0s 123us/step - loss: 0.5818 - val_loss: 0.5188\n",
      "Epoch 13/100\n",
      "2716/2716 [==============================] - 0s 128us/step - loss: 0.5689 - val_loss: 0.5083\n",
      "Epoch 14/100\n",
      "2716/2716 [==============================] - 0s 129us/step - loss: 0.5496 - val_loss: 0.4967\n",
      "Epoch 15/100\n",
      "2716/2716 [==============================] - 0s 132us/step - loss: 0.5420 - val_loss: 0.4849\n",
      "Epoch 16/100\n",
      "2716/2716 [==============================] - 0s 129us/step - loss: 0.5312 - val_loss: 0.4775\n",
      "Epoch 17/100\n",
      "2716/2716 [==============================] - 0s 127us/step - loss: 0.5336 - val_loss: 0.4717\n",
      "Epoch 18/100\n",
      "2716/2716 [==============================] - 0s 127us/step - loss: 0.5226 - val_loss: 0.4646\n",
      "Epoch 19/100\n",
      "2716/2716 [==============================] - 0s 127us/step - loss: 0.5018 - val_loss: 0.4603\n",
      "Epoch 20/100\n",
      "2716/2716 [==============================] - 0s 127us/step - loss: 0.5055 - val_loss: 0.4555\n",
      "Epoch 21/100\n",
      "2716/2716 [==============================] - 0s 130us/step - loss: 0.4972 - val_loss: 0.4496\n",
      "Epoch 22/100\n",
      "2716/2716 [==============================] - 0s 129us/step - loss: 0.4945 - val_loss: 0.4465\n",
      "Epoch 23/100\n",
      "2716/2716 [==============================] - 0s 128us/step - loss: 0.4963 - val_loss: 0.4423\n",
      "Epoch 24/100\n",
      "2716/2716 [==============================] - 0s 124us/step - loss: 0.4931 - val_loss: 0.4383\n",
      "Epoch 25/100\n",
      "2716/2716 [==============================] - 0s 129us/step - loss: 0.4893 - val_loss: 0.4348\n",
      "Epoch 26/100\n",
      "2716/2716 [==============================] - 0s 127us/step - loss: 0.4748 - val_loss: 0.4317\n",
      "Epoch 27/100\n",
      "2716/2716 [==============================] - 0s 128us/step - loss: 0.4755 - val_loss: 0.4297\n",
      "Epoch 28/100\n",
      "2716/2716 [==============================] - 0s 126us/step - loss: 0.4647 - val_loss: 0.4272\n",
      "Epoch 29/100\n",
      "2716/2716 [==============================] - 0s 127us/step - loss: 0.4637 - val_loss: 0.4243\n",
      "Epoch 30/100\n",
      "2716/2716 [==============================] - 0s 129us/step - loss: 0.4611 - val_loss: 0.4202\n",
      "Epoch 31/100\n",
      "2716/2716 [==============================] - 0s 126us/step - loss: 0.4639 - val_loss: 0.4187\n",
      "Epoch 32/100\n",
      "2716/2716 [==============================] - 0s 130us/step - loss: 0.4587 - val_loss: 0.4148\n",
      "Epoch 33/100\n",
      "2716/2716 [==============================] - 0s 127us/step - loss: 0.4464 - val_loss: 0.4125\n",
      "Epoch 34/100\n",
      "2716/2716 [==============================] - 0s 128us/step - loss: 0.4564 - val_loss: 0.4097\n",
      "Epoch 35/100\n",
      "2716/2716 [==============================] - 0s 130us/step - loss: 0.4470 - val_loss: 0.4090\n",
      "Epoch 36/100\n",
      "2716/2716 [==============================] - 0s 128us/step - loss: 0.4357 - val_loss: 0.4095\n",
      "Epoch 37/100\n",
      "2716/2716 [==============================] - 0s 131us/step - loss: 0.4323 - val_loss: 0.4089\n",
      "Epoch 38/100\n",
      "2716/2716 [==============================] - 0s 130us/step - loss: 0.4300 - val_loss: 0.4071\n",
      "Epoch 39/100\n",
      "2716/2716 [==============================] - 0s 146us/step - loss: 0.4284 - val_loss: 0.4069\n",
      "Epoch 40/100\n",
      "2716/2716 [==============================] - 0s 132us/step - loss: 0.4283 - val_loss: 0.4075\n",
      "Epoch 41/100\n",
      "2716/2716 [==============================] - 0s 135us/step - loss: 0.4184 - val_loss: 0.4069\n",
      "Epoch 42/100\n",
      "2716/2716 [==============================] - 0s 132us/step - loss: 0.4314 - val_loss: 0.4045\n",
      "Epoch 43/100\n",
      "2716/2716 [==============================] - 0s 125us/step - loss: 0.4287 - val_loss: 0.4029\n",
      "Epoch 44/100\n",
      "2716/2716 [==============================] - 0s 127us/step - loss: 0.4177 - val_loss: 0.4016\n",
      "Epoch 45/100\n",
      "2716/2716 [==============================] - 0s 126us/step - loss: 0.4163 - val_loss: 0.4017\n",
      "Epoch 46/100\n",
      "2716/2716 [==============================] - 0s 123us/step - loss: 0.4145 - val_loss: 0.3984\n",
      "Epoch 47/100\n",
      "2716/2716 [==============================] - 0s 124us/step - loss: 0.4125 - val_loss: 0.3965\n",
      "Epoch 48/100\n",
      "2716/2716 [==============================] - 0s 126us/step - loss: 0.4058 - val_loss: 0.3968\n",
      "Epoch 49/100\n",
      "2716/2716 [==============================] - 0s 124us/step - loss: 0.4008 - val_loss: 0.3959\n",
      "Epoch 50/100\n",
      "2716/2716 [==============================] - 0s 133us/step - loss: 0.4174 - val_loss: 0.3930\n",
      "Epoch 51/100\n",
      "2716/2716 [==============================] - 0s 125us/step - loss: 0.3982 - val_loss: 0.3926\n",
      "Epoch 52/100\n",
      "2716/2716 [==============================] - 0s 126us/step - loss: 0.4040 - val_loss: 0.3920\n",
      "Epoch 53/100\n",
      "2716/2716 [==============================] - 0s 126us/step - loss: 0.4036 - val_loss: 0.3914\n",
      "Epoch 54/100\n",
      "2716/2716 [==============================] - 0s 125us/step - loss: 0.4013 - val_loss: 0.3917\n",
      "Epoch 55/100\n",
      "2716/2716 [==============================] - 0s 124us/step - loss: 0.3946 - val_loss: 0.3915\n",
      "Epoch 56/100\n",
      "2716/2716 [==============================] - 0s 133us/step - loss: 0.3872 - val_loss: 0.3904\n",
      "Epoch 57/100\n",
      "2716/2716 [==============================] - 0s 140us/step - loss: 0.3943 - val_loss: 0.3906\n",
      "Epoch 58/100\n",
      "2716/2716 [==============================] - 0s 137us/step - loss: 0.3964 - val_loss: 0.3918\n",
      "Epoch 59/100\n",
      "2716/2716 [==============================] - 0s 143us/step - loss: 0.3773 - val_loss: 0.3923\n",
      "Epoch 60/100\n",
      "2716/2716 [==============================] - 0s 128us/step - loss: 0.3878 - val_loss: 0.3915\n",
      "Epoch 61/100\n",
      "2716/2716 [==============================] - 0s 127us/step - loss: 0.3875 - val_loss: 0.3906\n",
      "Epoch 62/100\n",
      "2716/2716 [==============================] - 0s 131us/step - loss: 0.3845 - val_loss: 0.3863\n",
      "Epoch 63/100\n",
      "2716/2716 [==============================] - 0s 130us/step - loss: 0.3693 - val_loss: 0.3863\n",
      "Epoch 64/100\n",
      "2716/2716 [==============================] - 0s 132us/step - loss: 0.3807 - val_loss: 0.3853\n",
      "Epoch 65/100\n",
      "2716/2716 [==============================] - 0s 128us/step - loss: 0.3868 - val_loss: 0.3847\n",
      "Epoch 66/100\n",
      "2716/2716 [==============================] - 0s 129us/step - loss: 0.3889 - val_loss: 0.3822\n",
      "Epoch 67/100\n",
      "2716/2716 [==============================] - 0s 130us/step - loss: 0.3829 - val_loss: 0.3840\n",
      "Epoch 68/100\n",
      "2716/2716 [==============================] - 0s 125us/step - loss: 0.3885 - val_loss: 0.3833\n",
      "Epoch 69/100\n",
      "2716/2716 [==============================] - 0s 126us/step - loss: 0.3776 - val_loss: 0.3814\n",
      "Epoch 70/100\n",
      "2716/2716 [==============================] - 0s 127us/step - loss: 0.3910 - val_loss: 0.3828\n",
      "Epoch 71/100\n",
      "2716/2716 [==============================] - 0s 125us/step - loss: 0.3788 - val_loss: 0.3812\n",
      "Epoch 72/100\n",
      "2716/2716 [==============================] - 0s 130us/step - loss: 0.3804 - val_loss: 0.3817\n",
      "Epoch 73/100\n",
      "2716/2716 [==============================] - 0s 126us/step - loss: 0.3789 - val_loss: 0.3818\n",
      "Epoch 74/100\n",
      "2716/2716 [==============================] - 0s 127us/step - loss: 0.3812 - val_loss: 0.3802\n",
      "Epoch 75/100\n",
      "2716/2716 [==============================] - 0s 126us/step - loss: 0.3838 - val_loss: 0.3807\n",
      "Epoch 76/100\n",
      "2716/2716 [==============================] - 0s 128us/step - loss: 0.3783 - val_loss: 0.3819\n",
      "Epoch 77/100\n",
      "2716/2716 [==============================] - 0s 129us/step - loss: 0.3569 - val_loss: 0.3804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "2716/2716 [==============================] - 0s 127us/step - loss: 0.3673 - val_loss: 0.3810\n",
      "Epoch 79/100\n",
      "2716/2716 [==============================] - 0s 129us/step - loss: 0.3622 - val_loss: 0.3815\n",
      "Epoch 80/100\n",
      "2716/2716 [==============================] - 0s 125us/step - loss: 0.3818 - val_loss: 0.3807\n",
      "Epoch 81/100\n",
      "2716/2716 [==============================] - 0s 127us/step - loss: 0.3646 - val_loss: 0.3777\n",
      "Epoch 82/100\n",
      "2716/2716 [==============================] - 0s 125us/step - loss: 0.3775 - val_loss: 0.3752\n",
      "Epoch 83/100\n",
      "2716/2716 [==============================] - 0s 125us/step - loss: 0.3715 - val_loss: 0.3772\n",
      "Epoch 84/100\n",
      "2716/2716 [==============================] - 0s 124us/step - loss: 0.3672 - val_loss: 0.3778\n",
      "Epoch 85/100\n",
      "2716/2716 [==============================] - 0s 127us/step - loss: 0.3621 - val_loss: 0.3789\n",
      "Epoch 86/100\n",
      "2716/2716 [==============================] - 0s 129us/step - loss: 0.3722 - val_loss: 0.3792\n",
      "Epoch 87/100\n",
      "2716/2716 [==============================] - 0s 127us/step - loss: 0.3697 - val_loss: 0.3777\n",
      "Epoch 88/100\n",
      "2716/2716 [==============================] - 0s 126us/step - loss: 0.3577 - val_loss: 0.3762\n",
      "Epoch 89/100\n",
      "2716/2716 [==============================] - 0s 126us/step - loss: 0.3672 - val_loss: 0.3772\n",
      "Epoch 90/100\n",
      "2716/2716 [==============================] - 0s 125us/step - loss: 0.3530 - val_loss: 0.3773\n",
      "Epoch 91/100\n",
      "2716/2716 [==============================] - 0s 123us/step - loss: 0.3699 - val_loss: 0.3775\n",
      "Epoch 92/100\n",
      "2716/2716 [==============================] - 0s 128us/step - loss: 0.3559 - val_loss: 0.3785\n",
      "Epoch 93/100\n",
      "2716/2716 [==============================] - 0s 126us/step - loss: 0.3573 - val_loss: 0.3773\n",
      "Epoch 94/100\n",
      "2716/2716 [==============================] - 0s 127us/step - loss: 0.3643 - val_loss: 0.3762\n",
      "Epoch 95/100\n",
      "2716/2716 [==============================] - 0s 126us/step - loss: 0.3717 - val_loss: 0.3757\n",
      "Epoch 96/100\n",
      "2716/2716 [==============================] - 0s 125us/step - loss: 0.3491 - val_loss: 0.3765\n",
      "Epoch 97/100\n",
      "2716/2716 [==============================] - 0s 129us/step - loss: 0.3612 - val_loss: 0.3759\n",
      "Epoch 98/100\n",
      "2716/2716 [==============================] - 0s 129us/step - loss: 0.3605 - val_loss: 0.3741\n",
      "Epoch 99/100\n",
      "2716/2716 [==============================] - 0s 123us/step - loss: 0.3282 - val_loss: 0.3765\n",
      "Epoch 100/100\n",
      "2716/2716 [==============================] - 0s 125us/step - loss: 0.3552 - val_loss: 0.3763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9cd2d88240>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.fit(np.array(X_train), np.array(y_train), epochs=100, batch_size=100, validation_data=((np.array(X_dev),np.array(y_dev))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on all the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a quick function to train and store the models from all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all(X_train_list,X_dev_list,y_train_list,y_dev_list,classifier,vocab):\n",
    "    import pickle\n",
    "    y_predictions = []\n",
    "    y_tests = []\n",
    "    models=[]\n",
    "    for x in range(0,len(vocab)):\n",
    "        X_train,X_dev,y_train,y_dev=get_single_vocab(X_train_list,X_dev_list,y_train_list,y_dev_list,x)\n",
    "        classifier.fit(np.array(X_train),np.array(y_train))\n",
    "        s = pickle.dumps(classifier)\n",
    "        models.append(s)\n",
    "        y_pred = classifier.predict_proba(np.array(X_dev))\n",
    "        y_predictions.extend(y_pred)\n",
    "        y_tests.extend(y_dev)\n",
    "    return (y_tests,y_predictions,models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=0.001,random_state=0, solver='lbfgs')\n",
    "#rfc = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "#clf = LogisticRegressionCV(Cs=[0.00001,0.0001,0.001],cv=5,random_state=0, solver='lbfgs',max_iter=200)\n",
    "y_tests,y_pred,models=train_all(X_train_list,X_dev_list,y_train_list,y_dev_list,clf,vocab)\n",
    "print(log_loss(y_tests,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With models trained and stored in a list, we can then move on to processing the test data and applying these models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the pairs of vocab that appear in the test file\n",
    "vocab_pairs = ['{'+ x[0]+'|'+x[1]+'}' for x in vocab]\n",
    "#split the file to word arrays\n",
    "test_data=file_to_split_sentences('test.txt',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_test_by_vocab(test_data, vocab_pairs):\n",
    "#     vocab_test_array = []\n",
    "#     for x in range(0,len(vocab_pairs)):\n",
    "#         vocab_test_array.append([])\n",
    "#     for test_example in test_data:\n",
    "#         for ind,word in enumerate(vocab_pairs):\n",
    "#             if word in test_example:\n",
    "#                 vocab_test_array[ind].append(test_example)\n",
    "#     return vocab_test_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_test_vocab = split_test_by_vocab(modified_test_data,vocab_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function to convert the test data to ngrams, as it is in a different form than the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_to_ngram(test_array,vocab_pairs,N):\n",
    "    ngram_test = []\n",
    "    target_word = -1\n",
    "    vocab_index = -1\n",
    "    for test_example in test_array:\n",
    "        for x in vocab_pairs:\n",
    "            if x in test_example:\n",
    "                target_word=test_example.index(x)\n",
    "                vocab_index = vocab_pairs.index(x)\n",
    "                break\n",
    "\n",
    "        if target_word ==-1:\n",
    "            print('error, desired vocab not found')\n",
    "            return 0\n",
    "\n",
    "        # set number of previous and following words to capture\n",
    "        max_previous = target_word - math.ceil(N/2)\n",
    "        max_forward = target_word + math.floor(N/2)\n",
    "        ngram = [test_example[x] for x in range(max_previous,max_forward) if x != target_word]\n",
    "        lenition,eclipsis=check_mutations(ngram[math.ceil(len(ngram)/2)])\n",
    "        ngram.append(lenition)\n",
    "        ngram.append(eclipsis)\n",
    "        ngram.append(vocab_index)\n",
    "        ngram_test.append(ngram)\n",
    "    \n",
    "    return ngram_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_test = test_to_ngram(modified_test_data,vocab_pairs,5)\n",
    "ngram_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_ngram_to_vec(ngram_test,word2vec_model):\n",
    "#     vectorized_list=[]\n",
    "#     for ngram in ngram_test:\n",
    "#         vectorized_data = []\n",
    "#         for word in ngram:\n",
    "#             vectorized_data.extend(word2vec_model.wv[word]) \n",
    "#         vectorized_list.append(vectorized_data)\n",
    "#     return vectorized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = []\n",
    "X_dev_full =[]\n",
    "train_vocab_indices = []\n",
    "dev_vocab_indices = []\n",
    "y_train_full=[]\n",
    "y_dev_full=[]\n",
    "for x in range(0,len(vocab)):\n",
    "    X_train,X_dev,y_train,y_dev=get_single_vocab(X_train_list,X_dev_list,y_train_list,y_dev_list,x)\n",
    "    X_train_full.extend(X_train)\n",
    "    X_dev_full.extend(X_dev)\n",
    "    y_train_full.extend(y_train)\n",
    "    y_dev_full.extend(y_dev)\n",
    "    train_vocab_indices.extend([x]*len(X_train))\n",
    "    dev_vocab_indices.extend([x]*len(X_dev))\n",
    "print(len(X_train_full),len(train_vocab_indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_full_train(X_test_full,vocab_indices,models):\n",
    "    import pickle\n",
    "    results = []\n",
    "    for index,line in enumerate(X_test_full):\n",
    "        clf = pickle.loads(models[vocab_indices[index]])\n",
    "        results.append((clf.predict_proba(np.reshape(line,(1,-1)))[0][1]))\n",
    "    return results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train,X_dev,y_train,y_dev=get_single_vocab(X_train_list,X_dev_list,y_train_list,y_dev_list,1)\n",
    "# y_trial_pred = models[0].predict_proba(X_dev)\n",
    "# log_loss(y_dev,y_trial_pred)\n",
    "#X_train,X_dev,y_train,y_dev=get_single_vocab(X_train_list,X_dev_list,y_train_list,y_dev_list,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_results = predict_full_train(X_train_full,train_vocab_indices,models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_train_full,y_train_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dev_results = predict_full_train(X_dev_full,dev_vocab_indices,models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_dev_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_dev_full,y_dev_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_full,vocab_indices = vectorize_ngram(ngram_test,new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_full(filename,vocab,set_vocab,new_model,classifier):\n",
    "#     #get the pairs of vocab that appear in the test file\n",
    "#     vocab_pairs = ['{'+ x[0]+'|'+x[1]+'}' for x in vocab]\n",
    "#     #split the file to word arrays\n",
    "#     test_data=file_to_split_sentences(filename,5)\n",
    "#     #preprocess to add UNK token \n",
    "#     modified_test_data= vocab_preprocess(test_data,set_vocab,vocab_pairs)\n",
    "#     split_test_vocab = split_test_by_vocab(modified_test_data,vocab_pairs)\n",
    "#     for x in range(0,len(vocab_pairs)):\n",
    "#         ngram_test= test_to_ngram(split_test_vocab[x],vocab_pairs[x],5)\n",
    "#         X_test_single = test_ngram_to_vec(ngram_test,new_model)\n",
    "#         y_pred = classifier.predict_proba(X_test_single)\n",
    "#         #evaluate performance using log_loss\n",
    "#         print('Test loss: ',log_loss(y_train,y_train_pred),' Val loss: ',log_loss(y_dev,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_full(X_test_full,vocab_indices,models):\n",
    "    results = ['Id,Expected']\n",
    "    for index,line in enumerate(X_test_full):\n",
    "        clf = pickle.loads(models[vocab_indices[index]])\n",
    "        results.append(str(index+1) + \",\" + str(clf.predict_proba(np.reshape(line, (1,-1)))[0][1]))\n",
    "    return results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_results = predict_full(X_test_full,vocab_indices,models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_results[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(filename, results):\n",
    "    out_file = open(filename,'w')\n",
    "    for line in results:\n",
    "        out_file.write(line+'\\n')\n",
    "    out_file.close()\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file('linearregCV.csv',y_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
